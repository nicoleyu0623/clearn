Course Overview

## ch01 Course Overview

Hi! My name is Bogdan Sucaciu, and welcome to my course, Designing Event-driven Applications with Apache Kafka Ecosystem. I am a software engineer at Axual in the Netherlands where I'm taking part in building a streaming platform designed to share information in real time. Nowadays, companies need to react faster to customer needs by enabling real-time processing of events and providing feedback as quickly as possible. If we add to that the complexity of designing and managing distributed systems, everything may become very complex very soon. In this course, we're going to explore how you can take advantage of the benefits that event-driven systems are providing to us with Apache Kafka and the ecosystem around it. Some of the major topics that we'll cover include event-driven architectures and how we can get into the proper mindset for designing and developing them, Apache Kafka and its clients, and, finally, we will explore the streaming model. By the end of this course, you should be comfortable with diving into any discussion about Apache Kafka and how to design your own solution when it comes to integrating it. Before beginning the course, you should know that I will also present some applications written in Java, so having a basic understanding of the Java programming language would help, but it is not mandatory since the course is not focused on writing code. I hope you'll join me on this journey to learn the Apache Kafka ecosystem with the Designing Event-driven Applications with Apache Kafka Ecosystem course at Pluralsight.

## ch02 Experiencing the Impact of an Event-driven Architecture

### Introduction

Hello and welcome to my course, Designing Event-drive Applications with Apache Kafka Ecosystem. My name is Bogdan Sucaciu, and I'm a software engineer at Axual in the Netherlands. If you're interesting on how event-driven architecture is impacting modern applications and how you could implement it in your own company using Apache Kafka, then this course is for you. During this course, I'll present how Globomantics has successfully introduced an event-driven architecture in their organization. The core of the company is an online store where they sell custom t-shirts to different customers. A part of their business process consists of designing, producing, and delivering the products using automated processes where possible. In this module, I'm going to present a short description of common enterprise software architecture patterns. Then we'll have a quick look over messages, events, and commands and what the relationship is between them. Implementing event-driven architecture comes with a great amount of benefit, but it also presents some drawbacks. I will finish with presenting event storming, a proven approach to obtain domain knowledge of the business and transpose it into software development.

### Enterprise Software Architectures

When talking about enterprise solutions generally, a client-server structure is chosen over other models. The client and the server usually reside on separate hardware, so in order to access resources from the server, a communication channel will need to be established over some kind of network. A monolithic application usually uses a layered approach to structure the code where each layer has a specific functionality. The structure of a two-layer application would include a presentation layer represented by a user interface, which translates tasks into something that a user can understand. The data layer is where the information is stored and retrieved, usually a database or some other kind of storage solution. And the business layer where all the business logic resides. This layer controls the application's functionality by processing and evaluating commands given by the user interface. In this scenario, the business layer is composed of multiple modules such as orders, promotions, users, payments, and suggestions. When the number of features increases, so does the complexity of an application, and the cost of maintaining and developing it may become higher. In order to solve this issue, a new pattern emerged-- microservices. In a microservices architecture, the application is decomposed in a collection of loosely coupled services where each service or model solves a specific problem and communicates with other services through a simple, lightweight protocol. A microservices architecture can easily support multiple clients, such as web or mobile, without any additional overhead. Considering the previously defined models, transposing them into the microservices pattern would result in independent services. Orders will take and process orders. Suggestions, based on the current items that are in the cart, it can suggest some other items. Users, responsible for authorizing customers. Payments process and manage payments. And promotions, based on some specific rules, the customer can benefit from discounts. In order to ensure loose coupling, having a database per service is essential. Moreover, each service can use the type of the database which is the most suitable. The services are interconnected through some lightweight communication protocol often being REST APIs or remote procedure calls. To have a single point of entry for the client, an API gateway is required. An API gateway will forward all the requests from a client to the appropriate service, but it can include at the same time some features like authentication or monitoring. Microservices are a great solution for complex systems. But as you can see on the diagram, while the system gets more and more complex, so do the dependencies between services resulting in something called microservices hell. It all starts with a single request from a client, but in order to complete the request, many other requests are fired across the system making it very hard to track and manage your data throughout the system. To solve this issue, a different way of thinking was required. And so event-driven architecture came into place. Event-driven architecture, or EDA, is a software architecture pattern promoting the production, detection, consumption of, and reaction to events. In other words, in an event-driven architecture, everything revolves around events and not the data. There are many flavors of programming models that can be encountered in event-driven architecture. Among them the most popular is still the microservices pattern where focusing each microservice to handle certain types of events and decoupling them with a broker-based technology. The event-driven microservices architecture can be a great solution for many problems. Serverless applications and the now-popular cloud computing services Function as a Service can solve problems that require small, short-lived applications. The streaming model is very popular nowadays due to the fact that companies need to respond faster to a customer's needs. While streaming, the events are processed as they arrive in the system so you can consider a stream as a never-ending operation giving the possibility for companies to react in almost real time. Event sourcing implies storing the data as a sequence of events in re-creating the current state based on the log. Try to imagine a journal in which instead of changing the current balance every time you make a transaction, you write down all the transactions that you are making with money. You can always re-create the current state by following the transactions incurred from the beginning until the current moment. CQRS stands for Command Query Responsibility Segregation, and it works by separating the interfaces for read and write. In a traditional system, both reading the data and writing it are executed through the same set of entities. Using CQRS, the interfaces would be separated and would be exposed over different APIs. There are a lot of amazing Pluralsight courses which cover all the patterns presented on this slide, and I encourage you to explore all of them and decide which one fits you the best.

### Messages, Events, Commands

I would like to talk a little about messages, events, and commands because it's very easy to confuse these terms one with another. A message is a basic unit of communication, and it can be literally anything. It can be a string, a number, or a full-blown object. Messages can be described as a generic interface which has no special intent. In order to use something more meaningful, we have to introduce two more concepts-- events and commands. An event is a message which informs various listeners about something that has happened. A real-world example of an event would be publishing an advertisement on a pin board. The person that publishes the event would be called a producer, and then the interested parties, which are called consumers, would subscribe to that advertisement and then would react to the ad_posted event. Please note the semantics of the words used. An event should always be referred to in its past tense because it defines something that has already happened or something that has been already triggered. In contrast to an event, a command will present a targeted action by having a one-to-one connection between a producer and a consumer. For instance, ordering tacos can be viewed as a command. Why? Because when you're placing an order either by calling or even using your favorite home-assisting device, there is an interaction between systems that knows how to reach from one to another. It would be kind of strange to call a random number to order tacos, right? To exemplify this in code, it would be like calling a method from the user object named placeOrder and passing the order object, in this case tacos, as a parameter.

### EDA Benefits

I would like to point out a few key benefits of choosing an event-driven architecture. Firstly, all the components are decoupled one from the other. Just like classes in a programming language, services should have a low coupling between them. When components need to communicate, let's say service A needs to transmit some data to service B, it would then use a broker-based technology. A broker acts as a middleware by facilitating the transmission of data between two systems so the only thing that they need to know is the location of the broker. A direct communication between service A and service B would be forbidden since it would result in a tightly coupled situation. The second benefit of an event-driven architecture would be encapsulation. Events can be classified within different functional boundaries, and they can be processed under the same boundaries. For example, in a typical e-commerce system, we would have events related to analytics, payments, promotions, suggestions, or even authentication. There are clean boundaries between each type of event without having to worry about confusing them. Another important aspect is speed. An event-driven architecture is designed to run almost in real time by reacting to the incoming events as they arrive in the system. Let's compare a traditional system with an event-driven one and how they can react in a common scenario like handling notifications. In a typical system, a user would tell the application, Hey, let me know when you have this awesome T-shirt in stock again. When the stock increases, the user would probably have to wait for a scheduled job to scan for all the persons that want to be notified when this item gets back in stock. When that job is complete, the user will then be notified. In an event-driven scenario, the user will tell the system that he wants to be notified by sending an event called notifications_enabled. The moment the stock is increased, an event called a stock_updated is transmitted, and the user gets notified. The last point that I would like to talk about is scalability. In an event-driven architecture, scalability comes naturally allowing applications to accommodate a growing amount of work, as well as cutting down resources when they are not needed. Let's take, for example, a fraud detection service, which job is to detect if any transaction is considered to be suspicious and eventually stopped. When the number of incoming events is growing, things are becoming slower, and some of the requests may never be processed or, even worse, the whole system may freeze. The easiest solution would be the ability to horizontally scale up the fraud detection service to handle the growth in events.

### EDA Drawbacks

As any other pattern, event-driven architecture presents some drawbacks. Designing and developing event-driven applications requires a steep learning curve to get started because it poses some questions that traditional architectures by their nature have already given an answer to. Some of these questions may be, What can be considered the source of truth in an event-driven architecture? Another question might be, What if things go wrong at some point in time, and duplicative events occur? Are you ready to handle that? These questions would result in the second drawback of event-driven architecture. It may become very complex to maintain. Simple event-driven architectures were introduced many years ago, but systems evolved over time and enterprise solutions usually require complex flow of events, which may become hard to understand and maintain. In a traditional solution where requests coming into the system can be interpreted as a transaction, and if anything goes wrong during the processing of that request, the transaction can be reverted easily. In an event-driven system, a request would result in one or a multitude of events scattered across the system making it almost impossible to revert the actions triggered by them in case that something fails. Lineage is a very important topic when it comes to debugging your system. Events can get lost or corrupted, and because the applications are loosely coupled, it is almost impossible to determine from which system that event arrived. A simple solution for this would be pinning some kind of identifier to the event in each application that is passing through.

### Event Storming

When it comes to designing a new project or some new features that have been requested to an already-existing project, usually an architect or a senior engineer analyzes the requirement and then tries to map out into data models the developers can work with. In the end, an entire system will be built around those data models, and all the data will be most likely stored in a database. In an event-driven approach, events come first. In fact, as you will notice in the following minutes, at first you will not even care what your data will look like. The only thing that you care about is how events interact with your system and how they are going to be processed. A proven approach to model a business in an event-driven architecture is by using a workshop called event storming combined with domain-driven design. Event storming is used to model a whole business line with domain events as a result of a collaboration of different members of the organization. There are a few things that need to be put in place in order to facilitate this workshop. Firstly, you'll need a room big enough to fit the number of people that are going to participate in the workshop. As a suggestion, try to clear out the room of chairs and move the tables aside. It is an interactive workshop, and people will be required to move around freely. Secondly, you need to invite the right people. What does this mean? Usually, a knowledge of the business is distributed across the organization, and finding only the people which possess all the knowledge can be tricky. But don't worry. There is no hard limit on the number of persons that can be invited to the workshop. There is just a minimum amount of persons required, which is represented by a facilitator, the person that knows how the workshop works and gets the discussion going in the right direction, a couple of technical people, architects, developers, or from the user experience department, and, lastly, a domain expert, someone who knows perfectly how the business works internally and its processes. In order to map out the processes, you will need to provide unlimited modeling space. Usually a huge piece of paper taped on the wall works best. All the events will be written on sticky notes of different colors, so it would be good to have some packs of Post-its and markers available for everyone. And, lastly, the workshop may take an entire day or it can be scheduled across multiple days depending on the complexity of the business. So it is best to have some food prepared to keep people focused on the workshop and not on their stomachs. The sticky notes that will be used in event storming sessions correspond to a color scheme. The domain events should be marked on orange stickies, but what exactly is a domain event? A domain event is something that is happening in the system and is relevant to the business. Usually a domain event provokes a reaction, maybe even another event, and your system needs to react to them. Then we have policies marked on purple sticky notes. A policy represents a process occurred by an event, and it always starts with the keyword Whenever. For example, whenever a new account has been created, we send an email of confirmation. External systems marked with pink stickies are used to define any interaction with applications that are managed by someone else and you do not have control over it. An example of such a system would be an external payment provider like PayPal. Finally, we have commands, which are actions initiated by a user or by a system like a scheduled job. They are marked with blue stickies, and the only difference between a command and an event is the fact that commands usually reside at the beginning of an event flow triggering the event chain. Once the business model is complete and all the bottlenecks have been addressed, we can step into the solution space using domain-driven design. To model the software, we first need to identify the aggregate by logically grouping various commands and events together. The goal is to define structures that are isolating the related concerns from one to another. When all the aggregates have been identified, it is time to define the bounded context. Aggregate groups related behavior together, whereas bounded context group meanings allowing the use of the same terms in different subdomains. For instance, Received in the order system may have a totally different meeting compared to Received in the shipping system.

### Demo: Event Storming

In this demo, I'm going to simulate an event-storming session by modeling a very basic user checkout journey using domain events. And then we're going to define our software solution using domain-driven design. During this demo, I'm going to use a tool called Miro, and it will help me to simulate an event-storming session serving as the modeling space. The model should follow a timeline from left to right of different interactions that can occur in a business. We'll start by defining domain events that may occur. Typically, a checkout process consists of three main events, an item has been added to the basket, the delivery address has been filled, and the payment details have been submitted. Now let's ask ourselves if there are any other events that may occur from the already listed ones. Well, for instance, an item may be available in stock or not. If it is in stock, we are going to update the basket so a proper event will be triggered. Otherwise, an alert will be set when the item comes back in stock, which will then send a notification. The idea is to cover both the happy path and the exceptional conditions. The same thing can be applied for address and payment. An address may be valid or invalid, so we map the relative events. The payment process as well. It may result in an approved state or a rejected one. In the case that the payment succeeds, the order will be processed. Since we have mapped out most of the domain events, it is time to have policies in place. A policy may reside between events, but if the process is very straightforward, it is not mandatory to put it in. For instance, we can call the process that determines if an item is in stock or not the In Stock policy. The Alert Set event will result in calling a Send Notification policy, which will then trigger the Notification Sent event when the event gets back in stock. In some scenarios, we might be dependent on an external system. For example, in order to check if an address is valid or not, we have to call and Address Validation API. How do we know that we have to call that API? You guessed it right. The process would be defined in a policy. Sometimes a policy may result in some more complex operations such as payment processing. We found out that it is very difficult to implement different payment methods in our system, so we decided to choose an external payment service for anything else that is not a credit card transaction. In that case, we will trigger the proper event and process it accordingly. All the domain events, policies, and external systems are nicely mapped in our board, so we have only one thing left to do. We now need to define the commands that trigger all these chains. As mentioned in the previous video, the commands usually reside at the beginning of a chain. In our case, all the commands are user-initiated actions. Commands are written using present tense, so we would have Add Item to Basket, Insert Address, and Insert Payment Details. Now that our business is nicely modeled using event storming, we can step into the solution space by using domain-driven design. We start off by defining the aggregates which are marked using yellow sticky notes by grouping commands and events altogether. For instance, all the events that are related to the user basket can be grouped together with the Add Item to Basket command. We can do the same thing for the address where Insert Address command is grouped with the Address Validated and Address Rejected events. After all the aggregates are identified, we can now define the bounded context by grouping aggregates which are part of the same subdomain together. For example, all the aggregates which are identified from the first two chains of events can be considered the Order context. The next chain can be mapped out to the Payment bounded context. As part of another session, we may define another bounded context such as Shipping, and everything goes on until the entire business is modeled.

### Summary

In this module, we had a quick look at different model enterprise architectures and explained the difference between messages, events, and commands. We then had an overview of key benefits but also drawbacks of choosing an event-driven architecture. Finally, we modeled a user checkout process using the event storming practice and the domain-driven design. This is enough to give you a high-level overview of event-driven architectures. In the next module, we'll start diving into implementing basic solutions Using Apache Kafka.

## ch03 Building Your First Apache Kafka Application

### Introduction

Hi! My name is Bogdan Sucaciu, and I'm a software engineer at Axual in the Netherlands. In this module, we will have an introduction to the Apache Kafka ecosystem and why it is so popular today while explaining how Kafka works internally. Then we'll have a quick look over the most common clients while creating two very basic applications.

### Why Kafka?

Kafka is increasing in popularity every year since its inception and for good reasons. It is open-source and has a great community contributing to the project and developing an entire ecosystem around it. Kafka was originally developed in 2010 at LinkedIn and was subsequently open sourced in 2011. In 2012, the project had graduated the Apache incubator and became a top-level Apache project allowing the users of the software to use it for any purpose, to distribute it or modify it without a concern for any royalties. Companies became very interested in this product, and many have started to adopt it, including companies like Netflix, Uber, and Spotify, who are handling millions of messages per day. In 2017, the first stable version of Kafka had been released, and two years later, the major version has been bumped again. Secondly, Kafka has been built in Java, one of the most popular programming languages in the world. This ensures a high number of contributors are maintaining and developing the product, but it also has debugging when something goes wrong especially because there are many Java-oriented DevOps engineers on the hiring market compared to some other programming languages. Originally, Kafka was developed with Scala, but soon after, it switched to Java because of its popularity and ease of delivering new versions. Although they are two separate programming languages, they can run under the same platform, so there was no need for a Big Bang type of change. The Scala and Java classes are passed through a compiler, a Scala compiler for Scala classes and a Java compiler for Java classes. After the compilation step, both the Scala and Java classes result in something called bytecode. The bytecode resulted is the same so it can be run under the same Java virtual machine without knowing if the original code was Scala or Java. Kafka is fast. It can handle a very high load of messages per second without using a massive amount of resources. It achieves this using two main features. Firstly, there is no serialization or deserialization happening inside Kafka. The serialization process consists of converting an known object, for example, a car, into a stream of bytes in order to transmit it over a network or to store it in memory or even in your file on your hard drive. The reverse process is called deserialization, and it allows converting the stream of bytes back into the known object after it has been transmitted over the network or when it is retrieved from the hard drive. The problem with these two operations is the fact that they are expensive in the sense that it requires processing power and time to do it. When only a couple of hundred messages per day are serialized and deserialized, it would not represent an impact to the system. But try to imagine a scaling up to millions or even billions of messages per day. These operations would slow down the entire system, so Kafka does not handle anything else rather than bytes. All the data that is stored on the hard drive is using a well-known byte encoding format, so there is no serialization or deserialization happening inside Kafka. The same thing can be said about what Kafka is receiving and transmitting over the network. All the data that is passed into Kafka and received from it will be in a binary format. The second reason why Kafka can handle a very high throughput is by using a mechanism called zero copy. The goal is to store a message in a hard drive for it to be persisted for the long term and prevent losing the data in case of a blackout. In a typical application when a message is received over the network, it passes through the network card, and then it is stored in the Java heap, which, from the hardware perspective, resides on the RAM. The data will then be copied to the hard drive. The reason for using the RAM and not the hard drive for storage is the fact that hard drives are usually very slow when it comes to random data access. Kafka is handling these by storing the data sequentially and bypassing the Java heap resulting in a situation where the data is not copied from one component to another, hence the name zero copy. A bit disappointing is the fact that the zero copy feature is only available for non-TLS connections. The reason for doing this is because the transport layer security protocol is deeply embedded in the Java development kit, so bypassing the JVM heap is not possible in this situation. So increasing the security on your system by using encrypted communication would result in some performance throwbacks. The final reason for choosing Apache Kafka over other solutions is the fact that Kafka is more than a messaging system. Kafka is considered to be a distributed streaming platform. What does a distributed streaming platform include? Well, first of all, it can be used as a messaging system by using the publish/subscribe pattern. In this pattern, there are two types of applications, producers and consumers. Producers create and publish the events to the messaging system while consumers subscribe to those events and consume them. Kafka can also be used for storing the data in a distributed way. It supports clustering so data will be uniformly spread across systems, but it can also be replicated in case one of the systems fails, thus ensuring data will not be lost. The last process that a distributed streaming platform can perform is processing the events as they occur in the system by taking benefit of the streaming model. Companies should react as fast as possible to customers' needs, and by using streaming, all the incoming events can be processed in almost real-time.

### Kafka Architecture

When we're talking about Kafka, we're actually referring to a compound of two components. The main component is the Kafka broker, which deals with receiving, storing, and transmitting data. But a broker cannot live by itself. It requires a zookeeper in order to achieve a distributed state. A broker should always reside in a dedicated server with dedicated resources since it has a very high input/output load, which may affect other applications or the other way around. A broker is nothing more than a process which lives on top of the operating system. When a message is received across the network, the broker stores it into the hard drive of that machine. When that specific message has been requested, the broker will then copy it from the file system, and it will transmit it to the application that requested it. It is important to mention that everything is done at the binary level. Messages are nothing more than bytes received over a network. Having a single server may be enough for the development environment where load throughput and loss of data may not be an issue. But in order to ensure that your setup is fault-tolerant and highly available, then multiple brokers need to be added under the same context. This distribution is called a cluster. When a message is received, the broker will duplicate it, and then it will send it to its siblings to prevent the loss of data in case of failures. Having a cluster setup also increases the throughput by offering a uniform load across the brokers without overloading one while the others are idle. The main problem is that the brokers are independent applications and one has no idea about the others. To solve this issue, we need a zookeeper. Consider the zookeeper as a centralized service that all the brokers are connected to. A zookeeper maintains a list with all the brokers registered to it but also some configuration required by the brokers to synchronize. It helps the brokers to distribute the load evenly by electing leaders who are in charge of handling some specific categories of messages. Also, in case of a broker failure, it will announce it to the others in order to prevent sending data to a faulty broker.

### Demo: Kafka Setup

In this demo, I'm going to set up a Kafka cluster on my local machine consisting of two brokers and one zookeeper. Please note that this setup is considered suitable for the development environment, and it should not be used in the production environment. For a production environment, its components should reside on separate servers, and the number of them should be adjusted according to the best practices and needs. I'm on the Quickstart page of the Apache Kafka website. We start by downloading the latest release, which at the current time of recording is 2.2 .0. Now that the download has finished, we switch over to the terminal window, extract the files from the archive. The command is tar -xzf where x stands for extract, z means running the command through gzip, a compression tool, and f is the flag you use when you have an archive file, and, finally, the name of the file. We change our directory to the newly created one, and we can find inside a couple of folders. The bin directory contains a couple of shell scripts that can be used for different operations when interacting with Kafka. In this video, we're going to focus on four of them, starting and stopping a broker, alongside starting and stopping zookeeper. The config folder contains a properties file required when running different components. In this video, we're going to focus on the server.properties, which is the property file for the broker, and zookeeper.properties, which, you guessed it right, is the property file required by the zookeeper. The libs folder contains a lot of Java archives, also called jars, which are used by the script to run the applications. Finally, the site-docs folder contains an archive with useful information about Kafka and how to use it. When creating a Kafka cluster, we first start by running zookeeper. As you will notice in the next couple of minutes, when we start the broker, we need to define a property containing the zookeeper URL to which it connects. By default, the zookeeper server will listen for connections on port 2181. You can change that by changing the clientPort property inside the zookeeper.properties file. To start a zookeeper server, we run the zookeeper-server-start script from inside the bin folder, and we pass the properties file as an argument. We can see that the zookeeper has successfully started, and it's listening on port 2181 on our localhost address. Now that our zookeeper has started, it is time to prepare our brokers. For that we need to create two new config files. The only way to run two brokers on the same machine is by using two different ports. So I'm going to create a server-1 config and a server-2 config. Inside each file, we need to change three properties. The first property is the broker.id. Each broker should have a unique ID, so the first broker will have an ID of 1. The next property is called listeners. The listeners property is the address where the broker will listen for connections. Since we're not going to use an encrypted connection, the first broker will listen for plaintext on port 9093. The last property is called log.dirs, and it represents the directory where Kafka will store all the messages received. Before closing this file, I would like to show you the zookeeper.connect property, which describes where the broker can find the zookeeper to connect to. We save and quit. I have changed the properties for the second broker by using the ID 2, and the port where it will have several connections will be 9094. We are now ready to run the two brokers. Similar to starting the zookeeper server, we run the Kafka-server-start script inside the bin folder by passing the config folder that we have just created. The first broker will use the first config file, while the second broker will use the server- 2.properties file. After a couple of seconds, we can see that both brokers have successfully started.

### Topics

When we are talking within the Kafka ecosystem, the messages transmitted to and from the Kafka cluster are called records. Just as a regular message, a record consists of three parts, a key, a value, and a timestamp. Both the key and the value can be represented by any type of information. It can be a string, a number, or even a full-blown object. The default limit for the size of a message is 1 MB, but you should always structure your messages in order to be sent and retrieved efficiently. The timestamp represents the number when the message has been produced. If you don't specify a timestamp manually, then the producer will do it for you. All the messages transmitted in an enterprise can be categorized to make them easier to track and process. A category of messages is represented by a topic. Topics are stored in a broker, but a list of all topics is managed by the zookeeper. In this scenario, we have categorized our messages in three main topics, Account, Cart, and Payment. So when an orange message is received, it gets stored on the Account topic. The plum message is stored on the Cart topic, and the purple messages are stored on the Payment topic. Based on the way the broker is sending the data, there are two types of topics, delete and compaction topics. Delete topics, as the name is saying, is deleting the data based on some factors. The first factor is the size. If the topic grows too big, the broker will start to delete old messages cleaning up space. In this scenario, our maximum size for our topic is set to 20 KB. Each message has 5 KB, so in our topic, we can store a maximum of 4 messages at a time. When new messages arrive on the system, older ones are deleted ensuring the capability of the broker to receive new messages. By default, there is no size limit set to topic configuration. The second factor that can result in deleting a message inside the broker is time. By default, a broker is configured to store messages for seven days, and then it deletes them. In our scenario, we have set the retention time to one day. As you can see, we have already produced some messages on day one. On day two, all three messages are deleted and no longer available. Two more messages are produced on the same day, but since the broker is able to retain them for only one day, on day three, the messages produced on the second day are now deleted. The other option when it comes to handling data inside your broker is by using compaction topics. They work a bit differently compared to delete topics by mimicking the upsert functionality from a database. Upsert is the abbreviation for update/insert. When a new message is produced on a topic, the broker will check if another message with the same key already exists in there. In case it does, it will update the old record. Otherwise, it will just insert the new one. Let's consider that we have already produced three messages inside our topic. Each message has a different key, K1, K2, and K3, but the same value, V1. When a new message with the same key, K1, is produced, the old value gets updated, V1 becoming V2. If a message with a different key is produced, then it will simply insert it without affecting the others. The process repeats for every new message that is produced. Apache Kafka is a distributed system, so we need a way to spread the load evenly across multiple brokers and also replicate the data inside of topics. The way that Kafka solves this problem is by using partitions. The same topic is split into multiple partitions, each partition living in a separate broker. Of course, in case that the number of partitions is bigger than the number of brokers, multiple partitions will reside on the same broker. The most important thing to remember is that each partition will store different messages. In other words, the same message will not exist in different partitions unless it has been produced multiple times. In our setup, we have two partitions, partition 0, which resides in broker 1, and partition 1, which resides in broker 2. When new messages are arriving, if we don't have a custom partitioning strategy, then the broker will spread the messages evenly. The green message will be stored in partition 0 while the orange message is stored in partition 1. Having a higher number of partitions solves splitting the load across the brokers, but what about failures? How can we ensure the safety of our data if a broker goes down? The answer is replicated partitions. Just as before, our topic is split into two partitions, but now two extra partitions have been added. These partitions are replicas of the original ones, thus ensuring the integrity of your data if one of your brokers goes down. When a new message is produced on partition 0 on broker 1, a copy of that message will be automatically sent to the other broker on the replicated partition. The same operation happens if a message is produced on broker 2. This process will ensure that all the messages can be recovered if one of the two brokers fails. We have explored many possibilities when it comes to handling messages and topics. But, unfortunately, there is no magic formula to determine how you should structure them. The only thing that I can suggest, and I cannot emphasize this enough, is to know your data. Only you should be able to determine how big the messages should be, what key they should have, what the value contain, and what type of topics you should use.

### Kafka Producer

A producer is an application which creates and transmits events to the Kafka cluster. The Kafka producer serves as an adapter for transmitting the data processed by our application by dealing with all the low-level details regarding serialization, network connections, or encrypted communication. To exemplify how a producer works under the hood, let's consider a user-tracking web page, which based on the articles that the consumer hovers over, it will send an event to a topics stored in a Kafka cluster. This event will later be used to build a list of suggestions. For example, when the user hovers over the purple T-shirt, an appropriate message will be sent to Kafka. So how will things work on the producer application? Well, we'll start with a bunch of information, so these are details about the customer that triggered the event and some data about the article that he showed interest in. After we have gathered all this data, we need to create something called a producer record. A producer record represents an actual message that will be sent to Kafka. The producer record like any other message is composed of two main things, a key and a value. Let's consider that an appropriate key for this type of record would be the user ID that has triggered the hovered event. We can find that information from the user details object. The value would be represented by a single concatenation of the type of the article that the customer showed interest in, the color, and the design if any separated by a comma. I've mentioned earlier that Kafka cannot store data types. It can only store bytes. So if we're actually sending our record to Kafka, we need to convert it into a binary format. The way to do that is by using a key serializer and a value serializer. The key serializer will, of course, convert the key, while the value serializer will convert the value. Since both our key and our value are strings, we're going to use StringSerializers for both of them. Now that our producer record has been successfully converted in a binary format, we can now transmit it to Kafka.

### Demo: Kafka Producer

In this demo, we're going to integrate the user-tracking functionality with a Kafka cluster by creating a basic producer application. This application will be written in Java, so in order to follow along, Java 8 or a higher version is mandatory alongside Maven, a build and dependency management tool. Before actually starting to work on our producer application, we first need to create a topic that we want to send data to in Kafka. To do that, we're going to use one of the scripts downloaded with the Kafka release. This script is called kafka-topics, and we need to pass a couple of flags as arguments. The first flag is the create flag because we want to create a topic. And then we need to point out one of the brokers from the cluster by using the bootstrap-server flag. In this case, the address of the broker is localhost on port 9093. Next, we fill in some details about our topic. We want a topic with two partitions. This will help balance the load across brokers and a replication factor too, meaning that for each partition that we have created, we create another replica for it. In total, we would have four partitions, two normal partitions and two replicated partitions. Finally, we finished by specifying the name of the topic. Let's call it user-tracking topic. To check if the topic has been successfully created, we can use the list flag with the same script. Now that our topic is created, we are ready to create our producer application. I'm going to use IntelliJ to build the application, but any ID will be more than good. I have already created the project with some existing classes inside, so it's best to focus only on creating the Kafka producer. The project contains a couple of models starting by an Event which is composed of some User details and the Product that he hovered over. The User model is very basic containing only an ID, a username, and a date of birth of that user. Each Product has a Color, a ProductType, which can be a T-shirt or design, and a DesignType. In the EventGenerator class, we are simulating the user-hovered event by creating random data and then returning it using the Event model. Let's now start linking everything up and creating the producer. Before writing the code that will interact with Kafka, we first need to add Kafka Clients dependency in our POM. The latest version at the current time of recording is 2.2 .0. In the Main class, we start by creating an EventGenerator, which will create our random events. Next, we need to set up some properties required by the Kafka producer in order to run. The bootstrap.servers represents the network addresses separated by a comma where the producer can find the brokers. Since we're going to use a string as an object type both for the key and the value, the key.serializer and the value.serializer will be both of type StringSerializer. On the next line, our KafkaProducer gets instantiated, and then we can start generating our events. We will generate 10 random events each at an interval of 1 second. In order to create our producer record, we need to extract the key and the value from the event. The key will be represented by the UserId while the value will be a string concatenation composed of the product type, product color, and design type. Our producer record is now ready to be sent to Kafka. Before finishing up the application, we need to close the connection with Kafka to prevent memory leaks. Let's now run the application. As you can see on the console, different events are now generated and sent to Kafka.

### Kafka Consumer

When it comes to returning data from a Kafka cluster, the consumer API is the most popular client to do so. When new messages arrive on the listener topic, the Kafka consumer will receive them and process accordingly. A very important thing to mention here is the fact that a Kafka consumer is a pull mechanism compared to other solutions that use a push mechanism. This means that when a message is available on the Kafka cluster, the consumer will pull and process it along the application to consume messages in its own rhythm. This is a very important feature because it allows slow consumers to do their job without blocking the Kafka cluster in any way. The way that a Kafka consumer handles records is very similar to how a producer does, but in reverse. Previously, we have published a couple of messages in the user-tracking topic on the Kafka cluster, so this time we're going to use the consumer to retrieve them. The consumer is constantly pulling from Kafka, meaning that it asks Kafka every couple of milliseconds, Do you have a new message for me? When a new message has arrived on the topic, the consumer will pull it, and it creates a consumer record. As you noticed, both the key and the value are encoded in a binary format so we need to convert them back into proper objects. Since we know that the key and the value were originally strings before encoding, we're going to use a StringDeserializer for both. After the key and the value have been deserialized, we have our objects back, and we can process them. For example, we might send them to a SuggestionEngine, which can now create a list of suggested articles for our customer.

### Demo: Kafka Consumer

During this demo, we're going to create a Kafka consumer that will ingest the messages from the user-tracking topic and then forward them to our SuggestionEngine for further processing and storage. I am back on my development environment. There's no pre-setup required before actual development since the topic has been created in the previous demo. The existing SuggestionEngine class does not have a concrete implementation, and it is using hardcoded values. Firstly, it extracts a type, color, and design from the message value while logging them on the console. Secondly, it retrieves the current user information persisted in the database. Next, we are adding the product that he has shown interest for to the existing list of preferences stored in the database. Based on the preferences list, we're going to generate a list of suggestions for that specific user. For simplicity, we're going to use hardcoded values since creating a proper SuggestionEngine will be out of the scope for this course. Let's start feeding to our SuggestionEngine events coming from Kafka. Just like a producer, we first need to add a Kafka Clients dependency in our POM. Back in the Main class, we start by adding some properties required by the consumer to run. The first property represents the network addresses separated by a comma where the consumer can find the brokers that he needs to connect to. The group.id is useful when we want to share the load of messages across multiple consumers without having to deal with duplicate messages. It is a bit more advanced topic, so I'll not detail about it in this course. But for now, you need to know that each consumer should be a part of a consumer group. If multiple consumers are part of the same consumer group, then they will share their load of messages, and they will act as a single consumer. We are finishing setting up the properties by specifying the deserializers for the key and the value. Since our messages are encoded strings, then we are going to use StringDeserializers. Next, we are instantiating the KafkaConsumer while passing the previously defined properties as arguments. The consumer needs to listen for incoming messages to a Kafka topic, so we subscribe it to the user-tracking topic. The same consumer can subscribe to a list of topics, but then we would need to implement some business logic to process the records based on the topic that the message has arrived from. To receive the records from Kafka, we need to ask the consumer to pull for 100 milliseconds. The pull duration represents the time that a consumer will keep the connection open with the broker and receives the record. After the duration is passed, we are able to access the consumer records and process them. In this case, we are going to integrate over all the messages that we have received and feed them to the SuggestionEngine. A pull operation will only happen once, so in order to keep the application running, I'm going to wrap this code in an infinite while loop. Let's run the application and see what behavior we are getting on the console. A bunch of useful information is displayed such as the topic that we subscribe to and the partitions that the consumer has been assigned to. In a separate tab, I'm going to start a producer application to show how those two interact one with another. Every time we are generating a new event with a producer, the consumer will receive it and process it accordingly.

### Summary

Apache Kafka popularity has grown massively in the last couple of years, and thousands of companies have started to adopt it. In this module, we have covered the basics of Kafka and the most common clients. We started by diving into why Kafka is so popular nowadays and its journey to becoming one of the most favored tools when it comes to enterprise messaging and streaming. Then we had a quick look at Kafka components by getting to know how brokers handle messages on a server and why it is mandatory to have a zookeeper among the brokers. Without categorizing the messages, the entire system would be very hard to maintain, so knowing how to work with topics is vital. We concluded the module by discovering the most basic clients, the Kafka producer and Kafka consumer. All the concepts presented in this module are done at a very high level, and I encourage you to explore further the capabilities of Kafka and its clients. In the next module, we are going to enhance our applications by using complex types on our messages instead of relying on plain string.

## ch04 Communicating Messages Structure with AVRO and Schema Registry

### Introduction

Hi! My name is Bogdan Sucaciu, and I'm a software engineer at Axual in the Netherlands. In this module, we'll explore how we can enforce complex types while transferring data from a producer to a consumer using Avro and Schema Registry. Avro is a serialization format which is completely open source, while Schema Registry is a tool that facilitates transferring data types between applications.

### Serialization Formats

Serialization is the process of translating data structures or object states into a format that can be stored or transmitted and reconstructed later, possibly in a different computer environment. This is the exact definition that I found on Wikipedia, but let's have a look at how the serialization process actually works. Our goal is to transfer some data, let's say from system A to system B. These two systems are separated by a network so they do not share the same memory. We are going to transfer some user data which contains information about a specific user present in system A. It is not possible to transfer the object as it is, so we need to transform it into a format that can be transmitted across the network. To do that, we are going to use a serializer. We pass our user object into the serializer, and it will transform it into proper transferable data. At the other end, we will need to reverse this process, so we're going to use a deserializer. We transfer our message across the network, and then the deserializer will kick in and transform our message back into the user object. This operation can be a two-way process, meaning that system B can transfer some data to system A. It can be completely different data, let's say a car object, but the process will be the same. Our car data is passed into the serializer. The serializer transforms it into a proper transferable state. Then it is passed across the network. And, finally, the deserializer will transform it back into the car object ready for processing. There are a couple of things that we need to consider when we choose a serialization format when transferring data between applications. Firstly, we have to wonder ourselves if the serialization format should be binary or not. Some serialization formats are binary by their nature, meaning that the data will be transmitted or stored as pure bytes. The other option would be working with serialization formats that use encoded text for those operations. Usually binary serialization formats use less bandwidth to transfer data because they are more compact, so implicitly they will be faster. The drawback is that the data is not human readable during the transfer process compared to non-binary serialization formats. The other thing that we need to consider while choosing a serialization format is if we can use schemas to enforce a strict data structure. Some data serialization formats allow having flexible structures while others enforce using only a specific one by using schemas. I've compiled a list of popular serialization formats for us to make a short analysis on whether they are binary and if they can use a schema or an interface definition language among them. The first serialization format is the well-known JSON. It is not binary since the data is transferred as encoded text, and the data structures are flexible, so there's no schema involved. Next, we have XML. It is not binary since XML data is using a well-known text format, but we can define schemas for our XML documents in order to enforce the specific structure to our data. XML schemas are not mandatory, but are a nice-to-have feature. YAML is commonly used for configuration files because of its minimal syntax. It is a text serialization format, and we cannot enforce a specific structure in the YAML files, so no schemas either. Avro has been developed within Apache's Hadoop project, and it serializes data in a compact binary format. It is using JSON-based schemas to define data structures. Protobuf, also known as protocol buffers, uses data serialization format created by Google designed to offer a simple and performant way of storing and interchanging data within systems. It is a binary protocol, and it is using an interface description language to describe data structures. Lastly, Thrift has been created by Facebook for "scalable cross-language services development. " Just as Protobuf, it is a binary format, and it is using an interface description language to define data structures.

### AVRO

Avro is a data serialization system, and it has many use cases. It offers rich data structures that can be stored within container files. Applications may use Avro for remote procedure calls by using a simple integration with dynamic languages such as Groovy, JavaScript, or even Python. It also offers code generation and improved performance on statically typed languages such as C# or Java. It is a binary serialization format, so all the data is compressed in a compact format making it lightweight compared to text serialization formats such as JSON or XML. Unfortunately, it is not human readable, at least not at the first glance. I have actually heard about people who worked so much with Avro that they are able to decode themselves the bytes and re-create the transmitted data just by looking at it. Avro uses JSON-based schemas to define data structures. These schemas are either embedded within the container files, or they can be transferred as separate objects. Let's see how the serialization and deserialization processes work with Avro. We start off with our user data to which we have an Avro schema that corresponds to the data structure that the user object has incorporated. Next, we pass both the user data and the Avro schema to the Avro serializer. This serializer based on the schema provided will transform the user data in bytes. Those bytes can be transferred across a network or stored on a hard drive. When we want to get our user data back, we need to use a deserializer. The bytes are passed on the deserializer among the user schema. Without it, the deserializer will not be able to transform those bytes back into the user data. Also, the correct schema needs to be passed. Otherwise, even with a slightly changed schema, the deserialization will fail. After the deserialization, the user object can now be used for further processing. This is what an Avro schema looks like. The file extension for Avro schemas is AVSC, but the content of it is JSON formatted. The schema starts with type. A schema's type can be either primitives or complex times. We'll have a look over all of them in just a minute. In this scenario, we have a complex object of type record. Next, we define a namespace, which together with the name it defines the full name of the schema. Since our schema represents a complex record, we define a list of fields. Each field has a name, a type, and some special attributes if it is the case. Both the userId and the username are of type string, while the dateOfBirth is of type int with a special attribute. We want to store a date, so the integer will represent the number of days from the UNIX Epoch, 1 January 1970. Just as I said before, Avro can deal with both primitive and complex types. As primitives, we can have null to represent when there is no value, Boolean to represent a binary state. Representing numbers can be done using int, long, float, or double. When we would like to store a sequence of dates, then we can use bytes. And, finally, string for storing a sequence of characters. Complex types can be represented by a record. Just as we've seen before, a record is a representation of a complex object composed of multiple fields. When we have a predefined list of values that needs to be enforced, we can use enums. Enums use an additional attribute called symbols where we pass in the values. For example, for a color enum, we have the blue and green symbols. Arrays for storing a list of values, maps are used to store key-value types of data where the keys are always of type string. Unions are nice to use when you have optional values. For example, a schema can either a null or a string value. And, finally, the fixed type can be used when we store a precise number of bytes. The list of all the data types can be found on the link below with complete details about each type.

### Demo: AVRO

In this demo, we are going to create the Avro schema for our user object used on the user-tracking topic as a key. After that we're going to generate the Java classes as they're going to replace the existing models in the producer and consumer projects. In my terminal window, I'm going to start by creating a schema file. Let's call it user_schema.avsc. We need to define a JSON structure for our schema, so we start by opening and closing curly brackets. The first attribute in our structure is the schema type. We want a complex object, so the type will be record. Next, we need to add the namespace. Since the generated class will be used both by our producer and consumer applications, we will use com.Pluralsight .kafka .model as our namespace. Finishing the metadata attributes, we need to specify a name for our schema, so let's call it User. We now need to define our user object fields. The first field is the userId, and it is of type string. The next field is the username, and just as the previous one, it is going to use string as a primitive type. The last field is the dateOfBirth. Avro does not have a type for handling dates, but we can use the integer type together with an additional attribute called logicalType. The logicalType will be a date, and Avro will store this data as the number of days from 1 January 1970. Our schema is complete and ready for generating the model class which will be used by our applications. In order to generate our Java class, we need to download a program called avro-tools that will parse our schema and create a model class. The avro-tools is packed in a JAR file, so we're going to use wget to download it. The latest stable version at the current time of recording is 1.8 .2. Now that our download is finished, we can generate our Java classes. The command is java -jar, the name of the jar that we have just downloaded, and then we need to add a couple of arguments called compile schema because we want to generate Java classes based on the schema. Next, we need to pass in the path where the schema can be found. In this case, it would be on the schemas directory. And, finally, the location where we want to persist our generated classes, which is our current directory. Let's inspect the results. As you will notice, the generated class is a bit more verbose than usual because it contains a lot of methods which are used by the Avro serializer for performance optimization.

### Schema Registry

Let's revise the application's landscape from the previous module. Everything starts with the user-tracking web page that sends events containing products that the user has shown interest in. Then our producer application prepares the formation and sends it to Kafka. After the messages reach the Kafka cluster, the consumer picks them up and sends them to the suggestion engine. This is how the data flows from the applications perspective. Now let's see what kind of transformations the data is passing through. We start with concrete objects that contain the information that we want to transfer. Before sending them to the producer, we need to extract the key and the value in a string format because our producer cannot handle complex types. After that, the producer will encode the text in bytes ready to be sent to Kafka. The consumer will receive the message in binary format, and it will decode it back into text. So both our key and the value reach the suggestion engine in a string format. Our goal is to transfer complex objects without relying on primitive types such as strings or numbers. To do that, we will need a Schema Registry. The Schema Registry is an application that handles the distribution of schemas to producers and consumers and storing them for long-term availability. The Schema Registry has an interesting solution for persisting the schema by using a Kafka topic to achieve that. Before actually diving into how the Schema Registry handles all the mentioned operations, let's first have a look over the subject name strategy. In the previous module, we have worked with the user-tracking topic, which has some user information as the key and some product data as the value. We want to store in this topic only data that feeds into these data structures. In other words, we would never want a car object to be persisted as the key in our user-tracking topic. We know that all the schemas are stored in a Kafka topic, but in order to retrieve the exact schema that we need, a proper mechanism needs to be put in place. Subject name strategy is achieving that by categorizing the schemas based on the topic that they belong to. The subject name will be the topic that we want schemas for, -key or -value. In this example, we would have the user-tracking-key and the user-tracking-value. I've simplified the previous diagram containing the application's architecture in order to show you how a typical message exchange will take place when Schema Registry is part of our landscape. We will focus on how the key on the user-tracking topic will be transferred, but the process for values is exactly the same. The key is represented by a user object. Before sending this object to Kafka, we need to serialize it into bytes. To do that, we are going to use an Avro serializer. Just as I mentioned in the previous video, we need a schema to serialize, so the serializer will ask the Schema Registry, Hey, can you give me the schema for the user-tracking topic key? The Schema Registry will find it in its own cache and send it. Now that we have both ingredients for serialization to take place, the process can be performed. The result will be a special message. The user object will be serialized into bytes, but it will be prepended with a unique identifier represented by the schema ID stored as well in a binary format. The message can now be sent to Kafka. When the consumer processes it, then the serializer will then take the schema ID present in the message. In order to obtain our object back, we need a proper schema. So the deserializer will ask Schema Registry, Hey, can you give me the schema which has this specific ID? The Schema Registry will then send the schema, and the deserialization can take place getting back the user data using complex types. Now you're probably wondering how the schemas actually end up in the Schema Registry in the first place. Well, in a nonproduction environment, the first application that interacts with the new topic can register the schema, but in a production environment, an administrator will have to upload them. He uploads the key and the value schemas for a specific topic into Schema Registry. The problem is that the Schema Registry stores the schemas in memory, and if something goes wrong, they are lost. The solution for this issue, Schema Registry is using an inbuilt producer, which transfers the schema in a special topic in the Kafka cluster. Now if the Schema Registry crashes, we can create a new instance which connects to the same Kafka cluster, and the inbuilt consumer can retrieve all the schemas stored in Kafka.

### Demo: Schema Registry

There are a few implementations of this Schema Registry, but in this demo, I'm going to use the one created by a company called Confluent because it is offering the best support for our schemas. Then we will upgrade the user-tracking producer and consumer from the third module by using Avro and Schema Registry for transferring records. Before diving into my terminal, I would like to let you know that Confluent Schema Registry is not completely open source. It is under Confluent community license, and it means that you can access the source code, modify it, or redistribute it with a single exception. You're not allowed to make a Software as a Service solution containing the Schema Registry that competes with their offering. Before tackling the changes required in the application, I'm going to set up the Schema Registry. The Confluent Schema Registry can be found on GitHub, so I'm going to use Git to clone this project. Now that the cloning process has finished, I can check out the latest stable tag, which is 5.2 .0. In order to run the application, I need, firstly, to compile and package it, and I'm going to use Maven for that. The command is mvn package. After a couple of seconds, the Schema Registry is ready to start. Similar to how zookeeper and brokers can be run, we can use the schema-registry-start script inside the bin folder while passing the schema- registry.properties file from the config folder as an argument. If everything goes all right, the Schema Registry will listen for a connection on localhost on port 8081. Now switching back to IntelliJ IDEA, I've imported both the user-tracking applications as a single project view to avoid switching tabs. Also, I generated the Java class for the prototype using Avro and copied it along with the User class inside both projects. The User class will serve as the key for our messages, while the Product will serve as the value. I'm going to start by modifying the producer application first. Because in my application I already have some classes called User and Product, I'm going to change their name to, let's say, InternalUser and InternalProduct to avoid confusion. Next, in our POM, we need to add two new dependencies, Kafka Avro serializer and Avro. In the Main class, we need to also change a couple of things. Firstly, we need the key and value serializers to use Avro ones instead of strings. Next, we need to add a new property represented by the schema.registry .url to which the serializers will connect to retrieve the schemas. Our producer is not going to send strings anymore, but full-blown Avro objects. Next, the key and the value for each event will be modified as well. The mapping will be one-to-one from an internalUser to and an Avro user object. Two last things before finishing up with the producer application is replacing the ProducerRecord types from a string to the appropriate object and choosing a different topic for our data. Let's call it user-tracking-avro topic. I've already modified the user-tracking-consumer application for it to use Avro object instead of plain strings, so I'm going to walk you through it. In the POM file just like for the producer, I've added the kafka-avro-serializer dependency and the avro dependency. On the Main class, I've changed the deserializer's type from a string deserializer to KafkaAvroDeserializer, and I've added a schema.registry .url property. The specific.avro .reader property must be set to true for the received record to be automatically cast to the appropriate type. The KafkaConsumer will now support a user object as the key and the product object as the value. The topic, as well, has been changed to user-tracking-avro topic. And, finally, the ConsumerRecords type has been changed from strings to Avro objects. Before running the applications, we don't really need to register the schema since that will be done automatically by the clients. This approach can be used in a nonproduction environment for simplicity. Let's now run the application. First, I'm going to start the consumer application. Now that it has successfully started, we can produce the messages using our user-tracking-producer application. Both of the applications are now transmitting and receiving data encoded in Avro format.

### Summary

A short module, we had a look on how systems can transfer and store data using serialization by looking over a few popular serialization formats and how they are categorized. Then we deep dived into how Avro can be used to define fixed data structures and the process of serializing and deserializing Avro data. We concluded the module by incorporating the Confluent Schema Registry in our applications architecture and upgrading the user-tracking application to use Avro for serialized data. In the next module, we'll start diving into the streaming model by creating a fraud detection application that inspects messages in almost real-time.

## ch05 Building Your First Streaming Application

### Introduction and Streaming Use Cases

Hello! My name is Bogdan Sucaciu, and I'm a software engineer at Axual in the Netherlands. In this module, we'll explore the streaming model and how Kafka offers first-class support for building streaming applications. Streaming has gained massive popularity in the last couple of years allowing companies to process their data in real time enhancing the customer experience. Going back to one of the diagrams from the first module where I presented a couple of solutions that can be implemented in event-driven architectures, we are now going to focus on the streaming part since Kafka offers out-of-the-box support for streaming operations. So what is streaming? Well, it's not about the videos that you watch on YouTube or any other video streaming platform, at least not in a complete sense. From a data science perspective, streaming is processing data events one by one as they arrive in our system. But then why when we are watching videos, we are saying that we are actually streaming them? Well, a video or even a live feed is actually composed of multiple pieces. When we are streaming a video over the internet, we are actually streaming every piece one by one and processing it. The processing part consists of displaying the video on our screen. Have you ever experienced network disruptions while watching a video? You are always able to rewatch the video until the disruption occurred, but no further. That is because the streaming model allows processing each piece individually. Let's see a couple of use cases where we can implement the streaming model. We have just seen what streaming videos actually means, but it is a vertical implementation of the streaming model, and it is not very interesting from a data science perspective. Focusing on streaming data, we can process actions that occur in our system giving the applications the capability to reply in real time to customer needs. As an example, we can consider that a user has entered his payment details, and our system needs to process the transaction. Data analytics is, again, a great domain for choosing streaming. Imagine a GPS system connected to the internet. In case of a traffic jam, you would like to be rerouted as fast as possible to avoid it. So implementing streaming for analyzing the traffic in real time would be a great solution. Stepping a bit more into the real world use cases, sensor detection is one of them. In case of fire, we'd like our system to respond in real time by activating the fire sprinklers and calling the appropriate authorities. Right now the world is more connected than ever to the internet through different devices that make our lives easier. But what about interconnecting those devices together and making them even more intuitive by using stream processing and even adding machine learning algorithm to enhance their capabilities? The last use case that we would like to talk about is alerting. What better way to react to different factors if not in real time? If some malicious user has accessed your social media account from a different device, I bet that you would like to know as fast as possible in order to take appropriate actions like changing your password.

### Designing a Fraud Detection Application

During this module, we're going to focus on implementing a fraud detection system, which has three basic rules for detecting fraudulent transactions going on in our web shop. The first rule consists of analyzing the order and rejecting any transaction that does not have a userId filled in. The second rule will analyze the payment details, and we will allow only orders with the number of items lower than 1000. Finally, another fraudulent transaction will be if the total amount listed on the order is greater than $10, 000. Let's see how such a system would look like in a traditional microservices architecture. Everything would start with the payment service. The payment service is responsible for validating the payment details and processing them accordingly. When the transaction enters the validation phase, the payment service will check if all the required data is present. After that, the payment service will delegate to a fraud detection application the responsibility to check if the transaction is not fraudulent. In order to save all the transactions done by a specific user, we would need to register all of them in a database. So every time a new transaction is passing through the fraud detection application, we would need to persist it in a database. If all the business rules are applied and the validation is successful, then the transaction is marked as not fraudulent, and it will be processed accordingly. On the other hand, if the transaction fails to comply with at least one of the business rules, then the fraud detection application will reply with a failed response. Since the validation phase is not successful, then the payment service will reject the transaction. Just by looking at this schema, we can identify two bottlenecks. Firstly, there is direct connection between our payment service and the fraud detection application. This would raise concerns when it comes to managing dependencies between systems because it will not be possible to run one without the other. The second bottleneck that we can identify is the database. Consider the fact that the fraud detection application will need to process thousands of transactions per second. At this rate running the database for each transaction will result in a high risk most times or the service may become unavailable at some point. Let's see how Kafka can help with this problem. We start off with our Kafka cluster to which we have connected a payment service capable of creating and transmitting transactions. On the other side, we have a payment processor, which is in charge to process all the transactions that arrive in that system. The payment service can be identified as a producer application and will deliver all the transactions in a topic called payments while the payment processor can be identified as a consumer, and it is expecting valid transactions from another topic called validated payments. Now it is our job to create a fraud detection service which is capable of identifying fraudulent transactions that arrive on the payments topic and rejects them while the genuine ones are passed over to validated payments topic. To achieve this objective, we'll need to plug in a consumer and a producer. The consumer will retrieve messages from the payments topic while the producer will transmit messages to the validated payments one. Part of our business logic is that we have to set up our business rules so that each transaction needs to pass through them. Let's see the journey of one transaction. The payment service will use the producer to transmit the transactions to Kafka on the payments topic. From there the consumer of the fraud detection application will pick it up and then forward it to the business layer. The transaction needs to pass all three rules in order to be considered valid. As you can see, only after it passed the third business rule, the transaction is categorized as genuine. The transaction is now ready to be transmitted to Kafka, so it will be picked up by the producer and dispatched to the validated payments topic. The consumer from the payment processor will detect that a new message has arrived on the listened topic, so it will pick it up and transfer it inside the application ready to be processed.

### Kafka Streams

In the previous clip, we've seen how a Kafka stream would fit in the big picture of our application's landscape. But what about its internals? A Kafka stream always connects to a Kafka cluster. It will not receive messages from any other places, so a very common pattern is by listening for incoming messages on one topic, let's call it topic A, and after the processing has been completed, it is going to send the processed messages to another topic, topic B. In order to do all these things, we will need a consumer on the left side and a producer on the right side. So far, there's nothing out of the ordinary, and we could have created this architecture by ourselves just by adding a consumer and a producer in a custom application which can perform some custom logic inside of it. The goal of Kafka Streams is to save us from all the trouble of setting up consumers and producers and abstract it all away in a compact format which is very easy to understand. During the stream processing, the event will undergo through a series of transformations. This chain of operations is called topology. The exact definition of a topology is an acyclic graph of sources, processors, and sinks. Let's dissect it a little bit to find out what this exactly means. We have said that it is an acyclic graph. In a graph, we have nodes and edges. The nodes are represented by what are called processors while the edges represent the line between them allowing the messages to go from one processor to another when the previous one has finished. It is acyclic because we do not want to process the same message over and over again. Otherwise, we would be stuck in an infinite loop. There are different types of processors. We will start with the already-known ones. The consumer represents a special kind of processor called source. It specifies where the stream will extract the data from in order to process it. On the other side, the producer that is placed at the end of the stream processing is called a sink. A sink processor will send all the data to the specified location. It is mandatory to have at least one source processor, but the number of stream and sink processors may vary. Each stream processor can perform a specific task, and it can be chained in order to achieve the desired result. Going back to the previous diagram, let's explore what would be a typical journey of a message which arrives on topic A. Every message which arrives on topic A would be processed in the order of arrival, so we're going to start with the plum message. The event will be read by the consumer, and then it will be redirected to the first stream processor. Let's consider that the first processor is a filter. All the plum messages that are going to be passing through this processor will be rejected from our stream. Every message is processed by the Kafka stream independently, so now it is time to process the green message. The source processor will read it from the Kafka topic and forward it to the filter. Our filter rejects only plum messages, so the message can safely pass to the next processor. Let's consider that this processor will be of type map. A map operation is represented by a transformation. In this case, all the messages that are undergoing this map operation are going to be transformed into blue messages. Moving to the last streaming processor, we consider it to be a counter. This processor will count all the messages that it receives based on their color. The problem is that every message will be processed independently to each other, and we need to somehow store the number of every message that is going through this processor. The answer to this problem is using a state store. The state store can be either ephemeral, meaning that once the application goes down, the data stored in it will be gone, or fault-tolerant by persisting the data in an external data source. The defaulting limitation is a fault-tolerant one by using an internal topic on the Kafka cluster as a storage area. The number of the blue messages is checked against this state store, and then the count marked as a purple message is sent to the sink processor. The producer will then forward all the messages that it receives to topic B. It is time to process our last message from the topic A. The blue message will be received by the consumer. Then the filter will allow it to go further. Since our message is already blue, the map processor has nothing to do, so the message will be forwarded to the counter. The count processor checks again with the state store how many blue messages have already passed and adds one. A new message is created and sent to the topic B with the updated value on the count. Can you figure out the logic of our entire stream processing? Pause the video for a couple of seconds and try to think about what we have managed to achieve using this streaming application. Okay, if we analyze the topology, we can see that we are counting all the messages except the plum ones from topic A and sending that number two topic B.

### Duality of Streams

In an event-driven architecture, often it is not enough to use only the streaming model. We would most likely need to use databases to store data. That is why when we are processing events, we can perceive a stream from two different perspectives. Firstly, a stream can be perceived as a stream. I know it sounds weird, but hang in there. When we are thinking about streaming, we are actually processing independent events with no connection whatsoever between them. For example, a user is ordering every day a new product. Each order is an independent event and it has no connection with the previous orders. We store these types of events in topics that you use delete as a cleanup policy. On the other hand, we can perceive a stream as a database table. In a database table, we persist only the latest state for some specific information. So perceiving streams as tables is all about processing evolving events. For instance, the balance of your bank account is the result of the sum of events. Some events may take your balance down while others may take it up, but they always have to rely on the previous events in order to calculate the current balance. In Kafka, we are storing these types of events in compaction topics. The best part of having this duality in place is the fact that we can always transform a stream into a table and vice versa. In order to transform a stream to a table, we can perform various operations like aggregating, reducing, or counting data. To achieve the reverse effect, we would just need to iterate over all the events from the beginning of time and store them as independent events.

### Stateless and Stateful Processors

Let's focus a little bit on stream processors. There are two categories of processors that we can use in Kafka Streams. The first category is stateless processors. They do not require state to perform their job, and they can process each event independently without worrying about the previous state. The second category, although I bet that you already guessed it, is stateful processors. Stateful processors require a state store in order to perform their operations. Kafka Streams offers a large number of transformations that can be performed in a stateless manner. The first operation is branch. We can use it to split our stream in multiple branches based on some business logic. Filter is used for rejecting messages based on a condition. Inverse filter is the opposite of a filter. Map can be used for transforming the messages from one type to another. FlatMap is useful for transforming one event to one or a multitude of events of the same type or different types. Foreach will just iterate over each event. Just a small warning about this operation. It is a terminal operation, so if a foreach is used, then we will not be able to use a sink processor anymore. If you just want to inspect the elements passing through the stream, then peek should be used. GroupBy is super useful when we would like to group events based on some elements like the key of the message or an attribute from the value. Finally, the merge function which can combine two streams in a single one. For the complete list of all the stateless transformations with complete details about them, please check the link on the screen. Part of the stateful operations can include aggregations like calculating the sum of all the transactions that have been posted in a topic, count messages with the same key. Joining streams or tables can be very useful when we would like to enhance some messages with information from different topics. Windowing can work with intervals of time on which we can perform various operations. Finally, we can even create our custom processors by using a lower-level API. In order to get more details about stateful operations on Kafka Streams, please check the link on the screen.

### Demo: Kafka Streams

In this demo, we are going to write our first Kafka Streams application by creating a fraud detection service which ingests messages from payments topic and will check them for any potential fraud by using three simple business rules. I'm back in IntelliJ. In the background, I have the Kafka cluster running along with the Schema Registry since we are going to use a complex object as the value. Our streaming application will consume messages from the payments topic and will send the verified transactions to the validated-payments topic. Each message is structured by having a simple string as the ID represented by the transactionId, while the value is a complex object representing an order object. The order is composed of a userId, the nbOfItems, and the totalAmount. Since our value is a complex type, I'm going to use Avro to represent it. I've pre-compiled the order class using Avro schemas, so I'm just going to copy it in my project. Next, we need to add a couple of Maven dependencies in our POM file. The first dependency is the kafka-streams. This dependency contains all the APIs that we are going to use for building the application. Since we have an Avro-generated class in our project, we will also need the Avro dependency. And, finally, we add two more Confluent dependencies for allowing our streaming application to work with the Schema Registry. Back in the main class, just like writing a consumer or producer, we need to add a couple of properties. Most of the configs you know already, but now I have used a different way of writing them. Instead of writing hardcoded strings, I've used constants which help us to avoid typos. You don't have to create your own constants since they can be found in the Kafka dependency for each client. For example, the BOOTSTRAP_SERVERS_CONFIG can be found in the Kafka Streams dependency while the SCHEMA_REGISTRY_URL_CONFIG can be found on the Kafka Avro serializer dependency. The APPLICATION_ID_CONFIG has multiple use cases. One of them being the group ID for the consumer. The other properties that we haven't covered yet are the DEFAULT_KEY and the VALUE_SERDE_CLASS_CONFIG. My streaming application uses both a producer and a consumer internally, so it needs a serializer and a deserializer to perform its job, hence the name SERDE. SER is the abbreviation of serializer, while DE is the abbreviation of deserializer. Now we can start working on the actual application. First, we need to create a topology for our streaming application. We do that by creating a StreamsBuilder, and then we call the build method on that object. After that we need to instantiate a Kafka Streams object by passing the newly created topology and the properties as arguments. The start method from the streams object will run the streaming application in a background thread. We finish the program by adding addShutdownHook, which will close all the connections to Kafka, and the application will be stopped. You have probably noticed that we haven't really defined our topology yet, so it is time to work on that. We are going to start by adding different nodes to the StreamsBuilder. Firstly, we need to specify the topic that we want to consume messages from. Secondly, we're going to print out all the messages that are received during the processing to help with debugging. Then it is time to add our business rules represented by three filters. The first filter will analyze if the userId is empty and will reject all the transactions which do not have a userId present. Next, we allow only orders with the NbOfItems less than 1000. And, finally, the threshold for allowing a payment to be processed is $10, 000. Anything that will be more than that will be rejected by our streaming pipeline. Just as an extra, I'm going to transform all the userIds by setting them to uppercase using the mapValues function. Before finishing up the topology, I'm going to print out all the transactions that have successfully passed the business rules and then will be produced to the validated-payments topic. Our streaming application is now complete, and we can safely run it. In the background, I'm going to run a producer, which will send a couple of messages to the payments topic. As you will notice, five messages have entered the streaming application, but only two of them have been actually valid. The first three messages have been rejected because one of the three rules has been broken. The last two messages have been successfully validated, and the userId has been capitalized.

### Summary

Streaming is a very powerful concept which gained massive popularity in the last couple of years. In this module, we started by exploring what a streaming model actually is, and then we designed a generic solution for one of its use cases. We continued by diving into Kafka Streams API internals and how we can interpret a topology. Then we explored different types of operations that can happen within a streaming processor. We concluded the module by creating our own fraud detection application using Kafka Streams while using three simple business rules which have been transposed to a topology. In the next module, we're going to explore further the streaming model by diving into KSQL, a completely different way of writing streaming applications with Kafka by using a SQL-like interface.

## ch06 Building a Streaming Application with KSQL

### Introduction

Hi! My name is Bogdan Sucaciu, and I'm a software engineer at Axual in the Netherlands. In this module, we'll explore the streaming model furthermore by discovering a new way of creating streaming applications using KSQL. Kafka SQL is a SQL streaming engine designed for ease of use by wrapping the Kafka Streams API in a friendly syntax. KSQL has been created by a company called Confluent, and it is under the Confluent community license. It means that you can access the source code, modify it, or redistribute it with a single exception. You are not allowed to make a Software as a Service solution containing KSQL that competes with their own offering.

### KSQL Basics

We already have so many options for creating streaming applications that can integrate with Apache Kafka, so why would we need another one? What can KSQL bring to the table? Well, let's start by having a look over a possible KSQL query. I know this is quite a controversial subject, but from a technical perspective, it seems quite straightforward, right? With this query, the biggest fan of pineapple_pizza can have all of them just for himself by diving into the pizza stream and selecting only the ones that have the type as pineapple. As an extra, we can also get some insight about the crust, size, and extra toppings that those pizzas have. Jokes aside, the main point is that even someone with no programming experience can understand and even create streaming applications with KSQL. This syntax is very similar to SQL, which makes it very easy to get started without having to worry about consumers, producers, serializers, and all the nitty-gritty of Kafka APIs. From a developer's perspective, we can structure the APIs in a pyramidal scheme. At the bottom, we have the producer and consumer APIs, which give us the most amount of flexibility. Somewhere in the middle, we have the Kafka Streams API, which abstracts away all the complexity of setting up consumers and producers and provides a great piece of operations that are usually performed in a streaming application. On top of the pyramid, we have KSQL. It has been designed to offer a friendly way of creating streaming applications by allowing a broader spectrum of people to work with it. If we look from bottom to top, we can see that the APIs are becoming more and more appealing for developers by abstracting the complexity in a nice format. On the other hand, if we go from top to bottom, the flexibility for creating streaming applications is increasing by not being limited by some very specific APIs. Which solution should you pick? Well, it depends on the problem that you want to solve. Different problems may require different solutions. For example, for a very simple data filtering application, you may want to use KSQL, while for some complex aggregations, Kafka Streams may be the right solution for you. The main point is to find a balance between ease of development and flexibility for your application. The second question that I would like to tackle is, How exactly does KSQL actually work? You will find that it has a very similar architecture to a generic database. Everything starts with the Kafka cluster since it is the core of the streaming platform. The streaming applications will be built inside the KSQL server. This server connects to the Kafka cluster and consumes and produces messages from it based on the instructions given by a user. In a nonproduction environment, the user would use a CLI in order to interact with a KSQL server. This CLI doesn't necessarily have to be on the same machine as the server because those two are using REST APIs to interact one with the other. The user will write instructions in a CLI. Then using the REST APIs, those instructions would be sent to KSQL server where the statement will be parsed, and the streaming engine will run it. Each query represents a different streaming application. But how exactly does KSQL run queries? Well, every query that we run on the KSQL server will be parsed to a topology and then run. On the left side, we have the query that we have talked about earlier. On the right side, we'll try to write a different Kafka Streams topology. The first line creates a stream of data which will be persisted on the pineapple_pizza topic. The Kafka Streams function to which it will be translated is the to function. The select statement will pick up only a few attributes of the pizza object, so the equivalent in Kafka Streams is the mapValues function. Next, we are specifying the topic from which we want to consume data. Using Kafka Streams API, we can achieve this using the stream function. Finally, the WHERE clause can be translated to a filter function. Let me order these functions a little bit to actually achieve a valid topology. In this case, we would have a source processor, which will consume messages from the pizza topic. Then to stream processors represented by the filter and mapValues function. And, finally, all the processed messages are then produced using the stream processor to pineapple_pizza topic. We've covered why we should use KSQL and also have how KSQL works under the hood. Now it is time to have a look over at when we should actually use it. KSQL is just another way of creating streaming applications, so all the use cases that can be covered by Kafka Streams are valid in here as well. Data analytics, monitoring, IOT, all of them are valid use cases for KSQL as well. More than that, with KSQL, it makes it very easy to actually view data. With Kafka Streams, we will need to query an application, set up a topology, and then run it. Quite some hassle for such a basic operation. With KSQL, everything is as simple as SELECT * FROM, add stream, and we are done. Finally, we can very easily manipulate and enhance the data stored in a topic. We can combine two data streams, change fields, or even eliminate them. Everything is just a query away.

### Windowing

We haven't really talked about the very common requirement during stream processing, which is the ability to work with time windows. We often need to perform some calculations in time boxes. For example, what is the average number of users visiting our website per hour? Or what is the total number of users which order a product per day? And many other examples like these ones. This feature is offered out of the box by Kafka Streams and, of course, it has been nicely integrated in KSQL as well. We don't need to worry about complex operations that need to be performed for time boxing your event flow. In order to illustrate different types of Windows, on the X-axis, I've marked intervals of 10 minutes. On top of the table, I've marked some messages received at different points in time. For example, the first purple message was received on the first minute, while the second purple message was received at minute 20. There are two types of messages, purple and green. And how Kafka Streams windows differentiates them is by looking at the key. In other words, all the purple messages have the same key. The same thing can be said about the green messages. All of them have the same key, but it is different from the purple ones. The first window type is tumbling. To illustrate how tumbling windowing works, let's consider 30-minute intervals. The first interval starts from the 0 to the 30 mark. In the first 10 minutes, we have 1 purple message and 1 green message. From 10 to 20, only 1 green message has been produced, while from 20 to 30, 3 purple messages have been produced. The next window starts from minute 30 and ends at minute 60. And just like in the previous ones, every message feeds in the proper time window. Having these windows in place, we can now see, for example, exactly how many purple messages we have received in every half-hour interval. Another way to timebox the events is hopping time windowing. We'll see in just a minute while it is called like this. We have the same interval of 30 minutes, which starts from the 0 mark until the 30 mark. In this scenario, I will illustrate only the purple messages, but the same process happens for the green ones as well. While the time is passing, the messages are matched within the time window. While tumbling, we have a new time window after the interval has passed, but with hopping, we will have time windows that will overlap. If we set the hop measure to 10 minutes, then every 10 minutes, a new time window will be created. This type of windowing interval is useful when we would like to count the number of messages in the last interval. For example, if we would like to find out how many transactions have occurred in the last 30 minutes in a fraud detection application, hopping time windowing is one of the best solutions to solve this.

### KSQL Syntax

Everyone that has encountered the SQL language will find statements used in KSQL strikingly familiar. Just as in any SQL language, we have two types of statements, data definition language, or DDL, and data manipulation language, or for short DML. DDL statements help us to define what the data should look like. In case of KSQL, we can create either a stream or a table from an underlying Kafka topic. When running one of these two statements, KSQL will update its internal metadata store with no effect whatsoever on any Kafka topic. Things evolve over time, so some of the streams or tables may become redundant. In order to keep things clean, we would also need a way to delete them. Drop stream can be used for deleting streams and drop table for deleting tables. There are two special queries which are part of this category that I would like to explain at the end of this section, which are CREATE STREAM AS SELECT and CREATE TABLE AS SELECT. DML is used for viewing and altering data or, rather, manipulating it. As part of this category, we have the SELECT statement, which helps us to visualize the content of a stream, an INSERT statement, which will allow us to add new messages to our data stream. CREATE STREAM AS SELECT and CREATE TABLE AS SELECT are also part of the DML category. The reason for being part of both categories is the fact that these statements define and manipulate data in one go. Let's take, for example, CREATE STREAM AS SELECT. Firstly, it will create a data stream which eventually will be represented by a topic we delete as a cleanup policy. Secondly, it will produce all the resulting messages from running the SELECT statement inside a newly created topic. So the data definition part is represented by the CREATE STREAM, while the data manipulation part is represented by the AS SELECT statement. Let's put all this newly acquired knowledge to work and re-create the fraud detection application from the previous module but with a twist. Now instead of processing only the valid transactions to another topic, we will do the opposite. We are going to create an alerting application which will analyze all the transactions from the payments topic. This time the business rules for detecting if the transaction is fraudulent will be slightly different. If the number of transactions done by a specific user is greater than 5 in a 10-minute interval window, we'll produce an alert containing the user ID in another topic called warnings.

### Demo: Alerting with KSQL

In this demo, we're going to dive into KSQL by first setting up a server and a command line interface for writing a streaming application. Then, we'll explore the syntax of KSQL by creating an alerting application which will consume the messages from the payments topic and will create alerts which will then be produced to the warnings topic. I am back on my terminal. KSQL is available on GitHub, so we can start off by cloning the project. It is just as simple as git clone and the URL on where we can find the project. By cloning the project, we have just downloaded the code, so we now need to compile and package it. Before compiling the code, we first need to change the version to the latest stable release, which is 5.2 .0 at the current time of recording. The command for compiling and packaging the code is mvn package. After a while, we can see that Maven has finished its job, and we can safely start the KSQL server and CLI. Before doing that, we need to adjust a property inside the ksql- server.properties file. By default, the KSQL server will try to find the brokers on localhost on port 9092, but our brokers actually listen on port 9093 and 9094. So with just a minor amend, we are ready to go. In the background, I have my brokers and the zookeeper running along with the Schema Registry since the payments topic will use another schema. First, we need to start with a KSQL server. Just as the previous component, we need to use the ksql-server-start script inside a bin folder and then pass as an argument the property file, which is located in the config folder. If everything goes alright, we can see that the server will listen for connections on port 8088. Now that the server has started, we can run our CLI. I'm in kind of an exceptional scenario here since both the CLI and the KSQL server run on the same machine, but typically these two would be run on different machines. The CLI start script can be found on the same bin folder as the server start script. By default, the KSQL CLI will try to connect to the server, which is on the same machine on port 8088, so no extra arguments need to be passed. In order to check if everything is alright, we can run a simple query like SHOW STREAMS and then add a semicolon. This query will show us all the data streams which KSQL is aware of and a couple pieces of useful information like the underlying Kafka topic and the value format. We can also do the same thing with topics. We can run the SHOW TOPICS query, which will list all the topics present in the Kafka cluster. You can see that there is a difference is number. On the streams part, we only have one stream, while on the topics part, we have registered three topics. That is because _schemas and payments topics are not yet registered in the KSQL metadata. Actually, the keyword is Registered. We can see that we have an entire column dedicated to this fact. We can clearly see which topics have been registered in the KSQL metadata. In order to register a topic as a data stream, we need to use the CREATE STREAM query. In this case, I'm going to call my stream ksql_payments. There are a couple of naming conventions that you can use. Some people are using the same name as the topic, while some of them prefer to prefix the topic with ksql. There is no right or wrong in this. It is just a matter of preference. We finish the query by specifying the topic and the value format registered in the topic. Before actually running this query, we need to make sure that the schema has been registered for this topic. Otherwise, we would receive an error. Great! Now we're ready to create our streaming query, which will analyze all the transactions from the payments topic and send alerts to the warnings topic. The query will be something like this, CREATE TABLE warnings. It is a table and not a stream because we are going to perform some aggregations on the stream. If you remember from the previous module, by aggregating data from a stream, we obtain a table. On the next line, we have AS SELECT userId and COUNT function. These will model what our value will look like. In this scenario, the value will be composed of the user ID and the number of transactions done by that specific user in the last 10 minutes. We specify the input stream, in this case ksql_payments. Then we need to set up our time window, which is the hopping window type of the size 10 minutes, and every hop will be of 1-minute intervals. So every 1 minute, we will get another time window. Since we are using an aggregate function, COUNT, we need to use a GROUP BY clause, which will group our messages by the user ID. We finish out the query by specifying that only if the number of occurrences is higher than 5, we should post an alert to the warnings table. All the queries are now running in the background, so it is time to test them. I'm going to print out all the transactions produced on the payments topic and all the alerts that will be posted on the warnings topic. In the background, I'm going to run a producer, which will produce a few dummy transactions. As you can see, there are more than five occurrences of the same user ID in the last couple of minutes. We can see an alert posted on the warnings topic.

### Summary

Customers, producers, Kafka Streams, and now KSQL. Can this thing get any better? We have discovered amazing tools for creating streaming applications, each one with different strengths and weaknesses. KSQL is recognized for its simplicity in creating streaming applications without the need of knowing complex programming languages. Windowing is another powerful concept which allows us to work with time windows. It has been so well integrated with Kafka Streams and KSQL that it is trivial to process data in time intervals. We finished the module by creating an alerting application with KSQL, which inspects in real time the number of transactions occurring in 10-minute intervals, and when a threshold is passed, an alert is generated. In the next module, we are going to explore how we can integrate our Kafka installation with known applications like SQL databases without creating a custom application by using Kafka Connect.

## ch07 Transferring Data with Kafka Connect

### Introduction

Hi! My name is Bogdan Sucaciu, and I'm a software engineer at Axual in the Netherlands. In this module, we're going to explore a different way of transferring data to and from Kafka by using Kafka Connect. Kafka Connect is another open source tool under the Apache license which can help us to avoid writing boilerplate code and custom applications by leveraging existing connectors which allow us to transfer data in a completely fault-tolerant way.

### Why Kafka Connect?

Kafka Connect is a very generic tool for transferring data, which theoretically has limitless extensibility by leveraging a few patterns that we are going to learn in the next couple of minutes. Before that, I would like to cover what Kafka Connect can bring to the table and the problems it actually solves. The first reason for choosing Kafka Connect is code reusability. With Kafka Connect, we need 0 lines of code. We're still going to need to write some configuration files, but that doesn't count as Code. Why is this a good thing? Well, let me present you a tale of two companies. You can think of them as the most generic companies ever. Due to new regulations, the software developers from these two companies have been given a new task of exporting all the data passing through Kafka to a database. The first company has chosen PostgreSQL for storing the data, while the other company has chosen MySQL as a perfect solution for this job. Both companies now need to build a new system that takes data from Kafka and transfers it to a relational database. Since the requirements are very similar, the solution will be as well. Both teams have decided to create a custom application which leverages the Kafka consumer API to extract the data from Kafka. In order to export it to the database, some custom written code is required while using the appropriate database driver. In the end, probably around 95% of the code written by both of the companies would be similar since both of them are using the same APIs and patterns. The interesting fact about this story is that it does not apply only to relational databases. In fact, the number of technologies that are used in an enterprise environment is actually limited. New frameworks and libraries pop up every day, but only a couple of them are reaching a mature enough state. Some other technologies that you may want to send data to and from Kafka are document-based databases like MongoDB or graph databases like Neo4J, search engines like Elasticsearch or Solr. Also, cloud services are very popular nowadays, so why not store your files in an AWS S3 bucket or even integrate your system with social media interfaces like Twitter? So how does Kafka Connect play its role into this? Well, for sure there are going to be cases when a company would like to create a data pipeline between a graph database or a relational one. They need to transfer all the messages from a cloud service or to feed a search engine with necessary data. In all these cases, Kafka Connect can help us to achieve this with almost 0 lines of code written. Kafka Connect is a suitable tool for transferring the data for both to and from Kafka. The second reason for choosing Kafka Connect is sort of connected to the first one. The first point was about reusing the same code that others may have written, but by doing this, you're also gaining another benefit by not reinventing the wheel. Some software applications may have a very specific way of handling data. And if any other application which connects to it does not use the same mindset, some serious performance penalties may occur, or things may become unstable. Let me exemplify this with the common tool used in almost every system, a database. Consider a software application, it can be of any kind, and a database. In order to persist some data to the database, we first need to open a connection. We then transfer the data. And, finally, we close the connection. Closing the connection after finishing the transfer is recommended for freeing up resources on the database site. Now let's envision the same scenario, but this time with Kafka in place. We are going to use a consumer application to transfer the messages from Kafka and persist them to a database. Below we are going to track the time spent for every message. We're going to neglect the time required for the message to arrive from Kafka to the consumer application. The first message is consumed. Then a connection needs to be opened. The message is persisted to the database, and then the connection closes again. The same process happens for the second message. The message is consumed, the connection is opened again, the message is transferred, and, finally, the connection is closed again. The problem with this approach is that we are obtaining a lot of overhead by opening and closing the connection every time a new message is consumed, and with a few hundred messages per second, which is kind of normal for Kafka, your database will not be very happy. Let's see the same architecture but this time with Kafka Connect in place. Kafka Connect consumes the first message, but it doesn't immediately transfer it to the database. It actually waits for more messages to arrive. These messages are stored in an internal buffer. And when the buffer is full, the connection to the database is opened. All the messages are persisted, and then the connection is closed. This approach of micro-batching the messages has actually reduced a lot of overhead compared to the consumer approach on which for each message a new connection has to be opened and closed. The last point that I would like to talk about is scalability. Since Kafka usually handles a very high throughput, a single machine usually is not enough for transferring data. Kafka Connect solves this problem by having two modes that you can run it on. The first mode is called standalone, and it is recommended for development purposes. Kafka Connect on a standalone mode will create a single instance of the application, which makes it very easy to get started with. The second mode that Kafka Connect can run on is the distributed mode. Just as with Kafka brokers, multiple instances of Kafka Connect can run across multiple machines sharing the load and providing failsafe mechanisms in case one of the machines goes down.

### Kafka Connect Architecture

In the previous video, I've talked about running Kafka Connect in multiple ways like standalone and distributed modes. Let's see how exactly Kafka Connect achieves that. Since we are already accustomed with the database scenario, we're going to continue with that. We have a single Kafka broker running on a virtual machine. In this example, I've chosen a virtual machine as the underlying system, but it can be anything from bare metal to containers. On another virtual machine, we would run our database. Again, it can be any type of database. The implementation is not really important in this scenario. Somewhere in the middle, we would have a third virtual machine in which will reside Kafka Connect. Kafka Connect is represented on the application level by a worker. A worker is nothing more than a process running on the operating system, which is responsible for executing different tasks. By running Kafka Connect on a standalone node, it means that we are going to run only a single worker, and all the operations and configurations will be handled by that single worker. If we would like to run Kafka Connect in a distributed mode, then the recommended way to do that is by having multiple virtual machines available. On each machine, we would then run a different worker. The interesting fact about this distributed configuration is that the workers are using Kafka to synchronize one with another. So in case one of the machines goes down, the other two workers can share the load across them. The problem with workers is that they are meaningless. They are just some processes waiting to do some job. By starting up a worker, we haven't really achieved anything of value. In order to give some meaning to them, we need to add connectors. Each application has its own way of connecting and transferring data. So for each type of application, we would need a separate connector. We would use one type of connector for a relational database, another one for graph databases, and another separate one for a search engine. A connector, though, is only a blueprint on how we can connect to a specific application, but it doesn't actually do that. Let's consider again the database scenario. On Kafka, we have topics. Let's say payments and orders. On the database side, we have tables. A connector can tell Kafka how to connect to a database, but it can't say, Hey, take all the messages from the payments topic and put them in the payments table. To achieve this behavior, we would need a task. A task is the runtime behavior of a connector. For each connector, we can have multiple tasks with a different configuration. In our scenario, we could have a task that consumes the messages from the payments topic and persists them to the payments table and another one which does the same thing but this time for orders. Previously, I've talked about Kafka Connect being able to share the load while running in distributed mode. The load is actually a running task. By having multiple tasks running, Kafka Connect can distribute them to different workers. For example, a worker can run the payments task while another worker can run the orders task.

### Connectors, Converters, Transforms

Although in the past I refer to connectors as a generic component, there are actually two types depending on how the data is flowing. If the data is transferred from another application to Kafka, then we need to use a so-called source connector. On the other hand, if you would like to transfer some data from Kafka, we would need a sink connector. As an example, if we would like to transfer data from a database table to Kafka, we can use a JDBC source connector. JDBC stands for Java database connectivity, and it is in fact an API which allows connecting and running queries with the database. On the other hand, if we would like to transfer our messages to an AWS S3 bucket, we can use an AWS S3 sink connector. You may be thinking about why we need a distinction between those two types of connectors since we need to connect to the same application. Well, as explained in the previous video, the ultimate resource that actually executes the transfer of messages is called a task, and the task is actually a combination of multiple things. One of them is a converter. Let's suppose that we want to transfer all the data from a database to Kafka. In order to do that, we need to iterate over every row in each table. The tasks are running, and using a JDBC source connector, we are able to pull in the row containing the data. The job of a source connector is to pull the data from the source and also to transform it into a generic format that Kafka Connect can work with. Now as you know, Kafka cannot accept anything else than bytes, and it's the producer's job to take care of this organization format. Kafka Connect solves this issue by using converters. The message is passed to the converters, which will transform it into from the Kafka Connect internal format to the format that will likely store it in the topic. In this example, I'm using a string converter, but there are many others available like JSON, Avro, or even Protobuf converters. After the message has been converted, we can safely produce it to Kafka. A similar process is happening while doing the reverse operation. We can see that this time the order of the components is a bit changed. The closest component to Kafka is the converter while the furthest is the JDBC sink connector. The connector will always reside closest to the software application we want to transfer data to, but the process of transferring the data is fairly similar to the source connector. The message is consumed from Kafka. Then it is converted from bytes using the proper converter type to the Kafka Connect internal representation. Then it is passed to the connector, which will be eventually persisted to the database. Kafka Connect can also do some processing while transferring the message, but, first, the message is ingested by the source connector. It is transformed into the internal presentation that Kafka Connect can understand. And then we can use the transforms to do some processing over it. The full name of this feature is actually SMT, which stands for simple message transforms. The keyword here is simple. This API has been designed to apply small transformations to the message, like removing certain fields or adding some metadata. We shouldn't use transforms for complex aggregations or joins. A more appropriate tool for that would be Kafka Streams. After the fields have been removed, the message can continue its journey throughout the converter and finally end up in Kafka.

### Demo: JDBC Sink Connector

It is time to put in practice all the knowledge that we've gathered about Kafka Connect. We'll see Kafka Connect in action by transferring all the messages from Kafka topic to my SQL database. We are going to use a JDBC sink connector for that. As prerequisites to follow along, you will need a MySQL database up and running on your computer. Also, you'll need to download and unzip the mysql-jdbc-connector for Kafka Connect and mysql-driver since they are not part of the original installation of Kafka Connect. Both of them are available on the links posted on the screen. I'm back on my terminal. The best part of this demo is the fact that we do not have to install anything else other than a downloaded connector and driver because Kafka Connect is part of the deliverables of Kafka and with the Kafka directory that we have downloaded in module 3. If we switch for a second to the bin folder, we can observe the fact that we have two scripts called connect-distributed and connect-standalone. The connect-distributed script can be used for running Kafka Connect in distributed mode, while connect-standalone can be used for running it on the standalone mode. In this demo, we are going to use the standalone mode. I think that we have already noticed a pattern with the previous components. Normally, we would have a start script located in the bin folder to which we need to pass a properties file located in the config folder. Let's see if Kafka Connect can be started using this pattern. Both the start script and the properties file are called connect-standalone. Oops! We can see that something is missing. Kafka Connect start script actually requires at least two arguments that need to be passed. The first argument should be the worker config, while the other arguments are the connector's config that we want to run. Let's see what we actually have in the connect- standalone.properties. We can see that we have the well-known bootstrap.servers property. Currently, it is going out to localhost on port 9092, but our brokers actually run on port 9093 and 9094, so we can just replace the existing value with the ones of our brokers. During the previous modules, you've probably noticed that I'm not actually putting all the addresses of the brokers, and usually I only pick one of them. The recommended way is to actually pass multiple broker addresses. Next, we have some properties which refer to the key.converter and a value.conversion. In this demo, we're going to stick with existing values since I haven't got the chance to demo anything with JSON yet. Then, we have some properties related to the offset. These properties are required by Kafka Connect in order to know exactly which messages have been actually transferred. In case that we want to stop a task and then start it again, by using offsets, Kafka Connect will be able to resume from the last transferred message. The last bit that we need to tackle is plugins. Since the default installation does not contain the JDBC connector, we need to specify a location where Kafka Connect can load plugins from. There are a few examples that we can use, so let's stick with usr/local/share/kafka/plugins. We save and create. We have finished with the worker properties. Now it is time to create the connector one. I'm going to call it mysql- connector.properties. This configuration is a bit more straightforward than the previous one. Firstly, we have a name for our connector. We can call it no mysql-connector. Then we need to specify the type of the connector. In this case, it is going to be a JdbcSinkConnector. Then we set the number of tasks which will run with this specific configuration. We can choose 1. The topic that we want to transfer messages from, I'm going to use the topic called orders on which we are going to produce a couple of order messages. By default, Kafka Connect will transfer all the messages to a table which has exactly the same name as the topic unless we specify a different naming convention. Then we have some connection details for the database. And, finally, we have the auto.create flag, which will automatically create the table in case it doesn't exist. We save and create. Before starting the Kafka connector task, let's actually move the connector and the driver to the directory that we've specified in the worker properties file. Each connector should reside on an individual folder, so we need to create a new one. Let's call it kafka-connect-jdbc. On this newly created folder, we need to copy the Kafka Connect JDBC connector and the MySQL driver. At the current time of recording, the latest version for the connector is 5.2 .1, and the latest version of the driver is 8.0 .16. We can now go back to the Kafka installation folder and start Kafka Connect. We use the start script from the bin folder and the worker and connector properties from the config folder. We can see that nothing really interesting is happening, and that is because we are not producing any messages. I'm actually going to show you a simple way to produce messages to Kafka by using a simple script that we already have. This script is called kafka-console-producer, and it is located in the bin folder. We need to pass two parameters. The first one is the broker-list, which represents the address of the brokers, and the second one is the topic that we want to produce to. And that's it. Anything that we type on this console, it will be directly produced to the orders topic. Now if I remember correctly, we've chosen a JsonConverter for Kafka Connect. That means we need to produce JSON messages. And since we are also setting the key and the value.converter .schemas as true, we actually need to produce a JSON with a specific format. This JSON has to include two fields, a schema and a payload, and it will look something like this. We are actually embedding the schema of the orders object inside the message. As you can see, it is way more verbose than an Avro message to which we only pass a schema ID. Let's actually produce this message and see the results. We can't really see any feedback from the console-producer, but let's check if the message has been persisted to the database. Using the MySQL interface, I'm going to select all the messages from the orders table. We can see that our message has been successfully persisted in a database.

### Summary

Developing integration tools for our system is not always a cool way to spend your time, but with Kafka Connect, everything becomes painless. We've seen what Kafka Connect can bring to the table and why it is a very powerful contender when it comes to integrating Kafka with different known software applications. Then we actually dived into Kafka Connect internals and learned about different configurations that we can use while setting it up. Finally, we have discovered how we can leverage different connectors for different types of applications but also what role converters and transforms play in the whole architecture and how they can make our lives easier. In the next module, we're going to explore how we can obtain the same functionality as we would normally obtain by using normal producers and consumers but over a REST interface. If we want to consume messages directly from a browser or produce them from a legacy application, everything is possible as long as we are able to make REST calls.

## ch08 Integrating Applications with REST Proxy

### Introduction

Hi! My name is Bogdan Sucaciu, and I'm a software engineer at Axual in the Netherlands. In this module, we are going to explore a new way of producing and consuming messages from Kafka. Kafka broker and Kafka Clients are independent repositories, and some of the clients are not always up-to-date in terms of features, not to mention about programming languages that do not have a Kafka client developed currently. Stick with me in this final module where we will learn about the necessity of having an implementation of the Kafka client and how we can work around if it is not yet developed.

### Kafka Protocol and Clients

Depending on your previous experience, you may be familiar with the OSI network model. The modern internet is actually based on this system. I'm not going to go into much detail, but just to be sure that we are on the same page, I'm going to explain it a bit. It is a layered system, and the levels are counted from below to the top. The first three layers represent the media layer, and it deals with all the underlying details of creating a network between computers. By that I mean both the physical infrastructure like routers and switches and the way to find a single machine inside a network via an IP address. The top four levels are called the host layer, and starting with the transport level, we can already talk about end-to-end communication. In terms of protocols, we have examples such as TCP and UDP at level 4, sockets at level 5, SSL or TLS and SSH at level 6, and at level 7, we have some more popular protocols like HTTP or FTP. Probably this is a bit weird for you. Why am I bringing up the OSI model in a Kafka course? Well, at first the Kafka protocol was developed as a binary protocol over TCP. By doing this, the clients were able to communicate with the Kafka broker over a non-encrypted transmission. A bit later, SSL support was added, and now we can actually decide which type we want to choose. If we would prefer a secure way of transmitting the data, then we can choose SSL. But this comes with a performance penalty. Now when it comes to the actual implementation, we are going to discover a few requirements. The protocol is a standard request/response method of communication. At first, the producer opens up a TCP connection to the broker, and without any additional handshakes, it is going to transfer the message as part of the request. In the case that it is needed, the broker can respond with the message acknowledgment as a response. The second requirement that a producer should meet is being asynchronous. A producer should be able to send a request even while awaiting responses for preceding requests. From a consumer perspective, things are pretty similar. The consumer opens up a TCP connection, and when it enters the pool, it is going to ask the broker as part of the request, Hey, do you have any new messages for me?, while the response is going to constitute the actual message. There is a large number of programming languages that have implemented Kafka Clients. Only the Java clients are maintained as part of the main Kafka project, but all the others are available through different open source projects. The good thing is that Kafka is increasing in popularity every day, and the number of programming languages which support Kafka Clients also increases. But today not everyone can use Kafka. There are a couple of cases that a workaround is required. This scenario is when you are developing an application in a programming language which does not have an implementation of the Kafka client. As I explained in the previous slides, implementing a Kafka client is not an easy task, and most probably you wouldn't want to do it all by yourself. In the other scenario, you may be working with a legacy application. For example, starting from the version 2.0, the support for Java version 7 has been dropped, and the minimum version required is 8. But what about applications that still run on Java 5 or 6? How can we integrate all these applications with Kafka?

### REST to the Rescue

REST architectures can have different implementations, but a few facts always have to be there. This style of communication requires a client/server model on where the server exposes some resources that can be either consumed or manipulated by the client. The most common implementation of REST is over HTTP. The server exposes so-called endpoints on where the client can either retrieve a specific resource, in which case the endpoint would be exposed using a GET verb, or send a resource to the server for processing. In this scenario, we would use the POST verb of HTTP. So how can we leverage this way of communicating by integrating it with Kafka? The answer is REST Proxy. It does exactly what it's called. It acts as a proxy server between Kafka and the other services which cannot benefit from the consumer or producer APIs. For example, if we would like to produce some message to Kafka, we can take advantage of the POST endpoint. The web app will initiate an HTTP connection, and it will send the message. REST Proxy will then take care of the serialization, and using a Kafka producer, it will send the message to Kafka. In terms of consuming, we can leverage the GET endpoint. A request is made by the service which desires to consume the messages. Then an embedded Kafka consumer goes into action and consumes the messages for the specific topic. REST Proxy will deal with all the serialization and then finally respond to the client with the consumed message.

### Demo: REST Proxy

I think it's time to present you the last demo of this course. We are going to start by setting up REST Proxy and integrate it with our existing Kafka cluster. Then we're going to use the curl command line tool to actually produce and consume some messages in the designs topic. During this demo, we're going to use the REST Proxy created by a company called Confluent, and it is under the Confluent community license. It means that you can access the source code, modify it, or redistribute it with a single exception. You're not allowed to make a Software as a Service solution containing the REST Proxy that competes with their own offering. Just like in the previous demos, we are going to stay mainly in the terminal. We're going to start by cloning the REST Proxy project. The command is git clone and the URL of the GitHub repository. The repository contains only the source code, so we need to package it. But before that, we need to switch to the latest stable version. At the current time of recording, the latest stable version is 5.2 .0. Now we are ready to compile and package the code. The command for doing that is mvn package. After a couple of minutes, we can see that the process has finished successfully. Before running the application, let's have a look over the config file in case something needs to be changed. We can see that the only property which is not commented out is the bootstrap.servers property. Our brokers can be reached on localhost on port 9093 and 9094, so we need to change the current list. PLAINTEXT is the protocol for producing or consuming messages, so it means that all the communication will be done in a non-encrypted way. If you would prefer to increase the security, we can choose SSL instead. But setting up the security for a Kafka installation requires a more complex procedure. The rest of the properties we can keep commented out since we are not going to use the more advanced features of REST Proxy. Okay, we're now ready to start up the application. REST Proxy can be started just like most of the applications from the ecosystem by running the startup script from the bin folder while passing the properties file from the config folder as an argument. After a couple of seconds, we can notice that the server has successfully started, and it is listening on localhost on port 8082. We can now switch to a new tab and start to produce a messages. For simplicity, I'm going to use curl since it is available for most distributions. In order to produce a message, we need to perform a POST request to a specific REST Proxy URL. The URL is composed of the domain and the port on where a REST Proxy is listening, /topics/, the topic that we want to produce dated to. Based on the content type, REST Proxy will determine which kind of messages will be produced on the topic. For producing JSON messages, we need to use the following type, application/ vnd.kafka .json .v2 +json, and, finally, the payload of the request, which has a specific format. The payload has to be a valid JSON containing a field called records. The records array contains JSON objects composed of a key and a value. Since we do not have a schema in this field, it can be passed by any kind of object. Finally, I'm going to parse the result through json_pp in order to get a nice output. The response contains some information about the key and value schemas, in this case both null, and some information about the partitions and the offsets about where the messages are stored. Offsets are an index on where we can find a specific message, and it is the mechanism used by Kafka for keeping order. If we would like to consume some messages, then we need to run multiple commands to set up a consumer. A consumer is a stateful application and will be bound to a specific REST Proxy instance. In order to start up a consumer, we'll need to make a POST request to the REST Proxy URL \consumers. As part of the URL, we need to pass a consumer group name in the case that we want to create multiple consumer instances. Let's call it designs-consumer-group. The Content-Type will be application/ vnd.kafka .vt +json. And the body of the request will contain three fields, the name for our consumer instance, let's call it consumer-1, the format of the message that is going to be serialized, in this case, it will be JSON, and the flag called auto.opposite .reset, which will be set to earliest. This will allow consuming all the messages from the beginning of the topic. Now that our consumer is up and running, we need to assign a topic to it. We can do that by, again, using a POST request. The URL is similar to the previous one, but we need to add some more parameters to the path, so the full path will be consumer/designs-consumer-group/instances/consumer-1, the name of the instance that we have chosen in the previous request, and, finally, subscription. The Content-Type header will be the same. And as part of the payload, we need to specify the topics that we need to assign to our consumer. In our case, it will be only one, designs. After all this preparation, we are now ready to consume our messages. To do that, we're going to use a GET request on consumers/designs- consumer-group/instances/consumer-1/records. The header that needs to be passed is now the Accept header, and it is of type application/ vnd.kafka .json .v2 +json. We can also parse the result to the json_pp utility to get a nicer output. After we receive the response, we can see all the messages produced in the topic.

### Summary

There is a very famous quote from a popular movie which I think fits like a glove to this module. This quote states, "I'm limited by the technologies of my time. " We've seen that not every programming language has a Kafka Clients implementation, which makes them difficult to integrate with. And since it is not an easy task to actually implement all the client APIs, we needed a workaround. And so REST Proxy came into place. REST Proxy allows us to perform the same operations that we would normally be capable of doing with Kafka Clients but this time over a REST interface. REST is a very popular style of communicating between different machines, and it has been widely adopted.

### Course Wrap-up and What's Next

Congratulations! I know that Apache Kafka is not one of the easiest subjects to learn out there, but I really hope that you have enjoyed my course. Let's see what we have actually covered in this couple of hours. We have started with a short introduction on event-driven architectures and how we can get into the event-driven thinking by using the event-storming workshop. Next, we started to explore Apache Kafka. We have looked into why it is such a popular yet powerful tool and what capabilities it has. Then we had a look over different clients and ways to integrate existing applications with Kafka. We have explored multiple ways of exchanging data from producers and consumers to Kafka Connect and REST Proxy. Finally, we have also discovered the streaming model by creating applications with different tools like Kafka Streams and KSQL. Although it was a lot of information, I barely scratched the surface on each of the components presented in this course, so there is much more to learn about. This course was designed to provide a high-level overview of some of the most popular tools out there that you can incorporate in your Kafka-driven system, but the list doesn't stop here. I recommend you explore further the Kafka ecosystem. In terms of Pluralsight courses, I definitely recommend that you watch the Getting Started with Apache Kafka course by Ryan Plant. In his course, Ryan covers more in-detail concepts around Kafka architecture and the consumer and producer API concepts that, as much as I would have loved to do, I haven't covered in this course. If you have any questions, don't forget that you can always ask me anything on the Discussion section on Pluralsight or just ping me on Twitter. Thank you for watching, and I hope that this was helpful for you and piqued your interest into the awesome world of Apache Kafka.